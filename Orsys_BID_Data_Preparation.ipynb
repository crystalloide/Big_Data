{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crystalloide/Big_Data/blob/master/Orsys_BID_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT : CERTAINES SOURCES DE DONNÉES KAGGLE SONT PRIVÉES\n",
        "# EXÉCUTEZ CETTE CELLULE POUR IMPORTER VOS SOURCES DE DONNÉES KAGGLE.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "6UkNvlzy8muH"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT : EXÉCUTEZ CETTE CELLULE POUR IMPORTER VOS SOURCES DE DONNÉES KAGGLE,\n",
        "# PUIS, N'HÉSITEZ PAS À SUPPRIMER CETTE CELLULE.\n",
        "# REMARQUE : CET ENVIRONNEMENT PORTABLE DIFFÈRE DE L'ENVIRONNEMENT PYTHON DE KAGGLE.\n",
        "# IL PEUT DONC MANQUER DES BIBLIOTHÈQUES UTILISÉES PAR VOTRE PORTABLE.\n",
        "\n",
        "titanic_path = kagglehub.competition_download('titanic')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "fzH-yyjN8muI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "fa1c06c3245237b07d584ac9984675953f90bcc2",
        "id": "fhurNBsc8muI"
      },
      "cell_type": "markdown",
      "source": [
        "# **Tutoriel général de préparation des données**\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "4bbfe635e70581311314c3d6111742a953d20e3d",
        "id": "mZOayC6m8muJ"
      },
      "cell_type": "markdown",
      "source": [
        "### Bienvenue dans ce tutoriel de préparation de données. Ce notebook est destiné aux débutants qui souhaitent apprendre à préparer correctement un jeu de données afin de le transmettre à un algorithme de machine learning. Je vous encourage à créer un fork de ce notebook, à tester le code et à l'améliorer !"
      ]
    },
    {
      "metadata": {
        "_uuid": "ab319b6d81ba9248843b1ad2758d3878a384c466",
        "id": "-TAyCIyJ8muJ"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://2s7gjr373w3x22jf92z99mgm5w-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/shutterstock_data_prep_-faithie.jpg)"
      ]
    },
    {
      "metadata": {
        "_uuid": "b578e8fc813355bbcf0e887c0120c4ddb5b14ab6",
        "id": "AYWWwBrb8muJ"
      },
      "cell_type": "markdown",
      "source": [
        "### Voici quelques ressources supplémentaires que vous pouvez consulter pour approfondir la compréhension des différentes techniques que nous allons voir dans ce cahier :"
      ]
    },
    {
      "metadata": {
        "_uuid": "50283e1cb80921d96364b582dafad1723512d221",
        "id": "B0fQB43m8muK"
      },
      "cell_type": "markdown",
      "source": [
        "[*Gérer les valeurs manquantes*](https://towardsdatascience.com/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca)\n",
        "\n",
        "[*Ingénierie des fonctionnalités*](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)\n",
        "\n",
        "[*Pourquoi l'encodage à chaud en apprentissage automatique ?*](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n",
        "\n",
        "[*Piège à variables fictives*](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
        "\n",
        "[*Encodage à chaud*](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding)\n",
        "\n",
        "[*Mise à l'échelle et Normalisation*](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)"
      ]
    },
    {
      "metadata": {
        "_uuid": "685dee8f16e05ef7eb8b90078aba46e802ecd61a",
        "id": "9av9L91-8muK"
      },
      "cell_type": "markdown",
      "source": [
        "## **Table des matières**"
      ]
    },
    {
      "metadata": {
        "_uuid": "386676783fb199ab10ceac466ed8ffd4aa206b86",
        "id": "4D2WZSz08muK"
      },
      "cell_type": "markdown",
      "source": [
        "1. [**Gestion des valeurs manquantes**](#missing)\n",
        "2. [**Ingénierie des caractéristiques**](#fe)\n",
        "3. [**Gestion des caractéristiques catégorielles**](#catvar)\n",
        "4. [**Mise à l'échelle des caractéristiques**](#scaler)"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_kg_hide-input": true,
        "trusted": true,
        "id": "ZXIfpn4r8muK",
        "outputId": "3b157ce6-89d7-453d-bd72-2b77d6aa9232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install chart-studio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Remplacement des imports obsolètes de Plotly\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "import cufflinks\n",
        "cufflinks.go_offline(connected=True)\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def draw_missing_data_table(df):\n",
        "    total = df.isnull().sum().sort_values(ascending=False)\n",
        "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
        "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    return missing_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chart-studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from chart-studio) (5.24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from chart-studio) (2.32.3)\n",
            "Collecting retrying>=1.3.3 (from chart-studio)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from chart-studio) (1.17.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->chart-studio) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->chart-studio) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->chart-studio) (2025.1.31)\n",
            "Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: retrying, chart-studio\n",
            "Successfully installed chart-studio-1.1.0 retrying-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "FrD_aPy98muL"
      },
      "cell_type": "code",
      "source": [
        "# Path of datasets\n",
        "titanic_df = pd.read_csv('../input/train.csv')\n",
        "titanic_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ddd3eba2b4f6366ab0d98f469a15cc0450a85941",
        "id": "EDK0oT2c8muL"
      },
      "cell_type": "markdown",
      "source": [
        "## **1. Dealing with missing values & outliers** <a id=\"missing\"></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "a831e5f105f5ac9cf9cab593833b0602991cbe5c",
        "id": "yKd5w2698muL"
      },
      "cell_type": "markdown",
      "source": [
        "### The first problem we encounter when preparing the data to pass it to a ML algorithm is the missing data. Indeed, most of dataset, especially dataset made of real world data, have missing values. For example, we can see here that our titanic dataset have some missing values :"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8253704df6dc5b0f0d74e35b661b8ec428bd8195",
        "id": "XQsYBDEG8muL"
      },
      "cell_type": "code",
      "source": [
        "missing_values = draw_missing_data_table(titanic_df)\n",
        "display(missing_values)\n",
        "missing_values[['Percent']].iplot(kind='bar', xTitle='Features', yTitle='Percent of missing values', title='Percent missing data by feature')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5392a9719179a729cf1ef15b50c59c3a438fb1b8",
        "id": "6LJoGCUV8muL"
      },
      "cell_type": "markdown",
      "source": [
        "### If a feature (a column of our dataset) do not have too much of it values missing, we can try to fill in these missing values. There are a lot of methods for filling in these values :\n",
        "\n",
        "- If there are too much missing data (>60%), you can drop the column :\n",
        "\n",
        "      titanic_df.drop('Cabin', axis=1, inplace=True)\n",
        "      \n",
        "- If there are only few missing data (1-2%) you can drop the rows which contain NAN :\n",
        "\n",
        "      titanic_df['Age'].dropna(inplace=True)\n",
        "\n",
        "### An better solution for a small amount of missing data is to study each obsevation case by case and replace missing values by looking at other features for this observation and try to find pattern between them to find out what can be the missing value.\n",
        "\n",
        "- In general, we don't want to loose data. A solution is to replace missing values by the mean or the median of the column. You should prefer median for columns which contains outliers that can skew the mean.\n",
        "\n",
        "      titanic_df['Age'].fillna(titanic_df['Age'].mean(), 1, inplace=True)\n",
        "      titanic_df['Age'].fillna(titanic_df['Age'].median(), 1, inplace=True)\n",
        "\n",
        "### The strategy of filling missing values depends a lot of the dataset and of your imagination ! So try to be creative, ask why these data are missing and how can I intelligently replace it ! Don't forget to try different replacing methods and to measure how the methods affect the performance of your model. Let's proceed with our titanic dataset :"
      ]
    },
    {
      "metadata": {
        "_uuid": "c8da6fc848902000a7b356dfa59f11b5d0b988fa",
        "id": "1AafYY5B8muL"
      },
      "cell_type": "markdown",
      "source": [
        "### There is only two missing values for the embarked column, let's try to replace it. Below is the distribution of Embarked according to Fare and sex, and the two observations with missing \"Embarked\" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5d71f475f8236bd0df71852f6a52e4889026562",
        "id": "5R2ETm5_8muL"
      },
      "cell_type": "code",
      "source": [
        "figure, axes = plt.subplots(1,1,figsize=(20, 8))\n",
        "plot = sns.catplot(x=\"Embarked\", y=\"Fare\", hue=\"Sex\", data=titanic_df, palette=('nipy_spectral'), kind=\"bar\", ax=axes)\n",
        "plt.close(plot.fig)\n",
        "plt.show()\n",
        "display(titanic_df[titanic_df['Embarked'].isnull()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96d8fa62c76b2e756e501444c99552dd4aa98829",
        "id": "jC_LLgOe8muM"
      },
      "cell_type": "markdown",
      "source": [
        "### Both passengers are female who paid 80 dollars as fare for their tickets. Moreover, they have the same ticket and cabin, so they probably had to board at the same place! According to the distribution above, the more probable embarked value for them is Cherbourg (C).. Let's replace these missing values:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "160380a05b7fe957b109da0290ee446ff933c85b",
        "id": "E9FXyEzg8muM"
      },
      "cell_type": "code",
      "source": [
        "titanic_df['Embarked'].fillna('C', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1680eb602baf52fec30bc425f0fca7d6a92e214d",
        "id": "hzSK8HKo8muM"
      },
      "cell_type": "markdown",
      "source": [
        "### For the age, we have 177 missing values, it's way too much to look a them case by case. We'll replace it by the median, altrough it may exist better solution which take into account other columns. If you find a solution for replacing missing age values which improve a lot the accuracy of your model, please share it in comments !"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3713234249422bee05b53f333ab127f551ca7934",
        "id": "Ofh27naw8muM"
      },
      "cell_type": "code",
      "source": [
        "titanic_df['Age'].fillna(titanic_df['Age'].median(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9a4bcc1d712e8a283f201afa5ffd777e311452a",
        "id": "5mbIc2jq8muM"
      },
      "cell_type": "markdown",
      "source": [
        "### Finally, cabin column is useful for finding the deck in which the passenger cabin is located, so we'll keep it. Let's replace missing values by 'U', meaning 'Unkown':"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "892039639905363f4e5e26736e83bfe2cb2671e7",
        "id": "hpdNJiYP8muM"
      },
      "cell_type": "code",
      "source": [
        "titanic_df['Cabin'].fillna('U', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "658a2ae96bf806569521ea616ec3cba49a452cb3",
        "id": "XUap7DBM8muM"
      },
      "cell_type": "code",
      "source": [
        "draw_missing_data_table(titanic_df[['Cabin', 'Age', 'Embarked']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fcdf726a3377e5f093b178fcae07651662eb3e0e",
        "id": "uFRKyI0F8muM"
      },
      "cell_type": "markdown",
      "source": [
        "## **2. Features engineering** <a id=\"fe\"></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "94def9bb6e7d8e66e0c47ded26ef10243883072c",
        "id": "UqLRgcVX8muM"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature engineering is the art of creating new features from already existing features or from knowledge about data. For example, with a small search on internet, we can identify that the first letter in the values of the cabin column corresponds to the deck of the boat in which the cabin is located. Thus, we can create a 'Deck' feature from the cabin feature. We can also create a Title column which corresponds to the title contained in the name of each passenger. Your imagination is the only limit to creation of features ! But keep in mind that you don't want to create features just to create features, the goal is to improve the accuracy of the model ! Here is few examples of features creation for the titanic dataset:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8daa8401d9316696a5bdecad95807c5e593649ec",
        "id": "t62lsXM68muM"
      },
      "cell_type": "code",
      "source": [
        "# Deck column from letter contained in cabin\n",
        "titanic_df['Deck'] = titanic_df['Cabin'].str[:1]\n",
        "titanic_df['Deck'] = titanic_df['Cabin'].map({cabin: p for p, cabin in enumerate(set(cab for cab in titanic_df['Cabin']))})\n",
        "\n",
        "# Title column from title contained in name\n",
        "titanic_df['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in titanic_df['Name']), index=titanic_df.index)\n",
        "titanic_df['Title'] = titanic_df['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
        "titanic_df['Title'] = titanic_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
        "titanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# Famillysize columns obtained by adding number of sibling and parch\n",
        "titanic_df['FamillySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1\n",
        "titanic_df['FamillySize'][titanic_df['FamillySize'].between(1, 5, inclusive=False)] = 2\n",
        "titanic_df['FamillySize'][titanic_df['FamillySize']>5] = 3\n",
        "titanic_df['FamillySize'] = titanic_df['FamillySize'].map({1: 'Alone', 2: 'Medium', 3: 'Large'})\n",
        "\n",
        "# IsAlone and IsChild column, quite explicit\n",
        "titanic_df['IsAlone'] = np.where(titanic_df['FamillySize']!=1, 0, 1)\n",
        "titanic_df['IsChild'] = titanic_df['Age'] < 18\n",
        "titanic_df['IsChild'] = titanic_df['IsChild'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d118e47ade8afeba0d947d24ac45eca4d1277338",
        "id": "21V0nF5f8muN"
      },
      "cell_type": "markdown",
      "source": [
        "### Once we've finished to create our new features, we can delete all useless remaining columns and print the first rows of our dataset:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f55056111610891accfda70fcc8fee07a84eefb",
        "id": "E76qypbY8muN"
      },
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], 1)\n",
        "titanic_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c6a2b251a8ae431def02d1c23ff6c0aa2792c370",
        "id": "Q_ybhk-08muN"
      },
      "cell_type": "markdown",
      "source": [
        "## **3. Handling categorical features** <a id=\"catvar\"></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "c153f7f9e43c2b84925f110ca597a59d92a47f42",
        "id": "7U_aaIMq8muN"
      },
      "cell_type": "markdown",
      "source": [
        "### As you can notice when looking at the dataset above, we've some categorical features in our data. Categorical features are features which values are non-numeric. Here, we have 4 of them : Sex, Embarked, Title, FamillySize. We need to transform it into numeric features in order to pass it to a machine learning algorithm.\n",
        "### A solution is to transform these features into numeric features can be to map the string values with numeric values. This solution is called label encoding. It can be done easily in Python using LabelEncoder class from scikit learn or map method of a pandas dataframe. For example, to label encode the Embarked column of the titanic dataset we have to transform letters corresponding to the embarked location into a number :\n",
        "\n",
        " - Embarked Cherbourg corresponds to 1\n",
        " - Embarked Southampton corresponds to 2\n",
        " - Embarked Queenstown corresponds to 3\n",
        "\n",
        "### The problem of doing this is that the algorithm may see this as a ranking between the three values. A better solution is to use hot-one encoding. Hot-one encoding means create one column per value of the source column (it is called a dummy variable), which take only binary values. For example, the embacked column dummy encoded gives us three columns : Embarked_C, Embarked_S, Embarked_Q. A passenger who embarked at southampton will for example have his Embarked_S column set to 1, whereas the two other embarked columns will be set to 0.\n",
        "\n",
        "![](https://www.renom.jp/notebooks/tutorial/preprocessing/category_encoding/renom_cat_onehot.png)\n",
        "\n",
        "### However, by doing this, we create a redundant column : with two of the three Embarked colomns, we can guess easily the value of the third column. For example, a passenger with Embarked_C and Embarked_S set to 0 will have necessarily his Embarked_Q column set to 1. In order to avoid that redundance, called the dummy variable trap, we must drop one of the column made when creating a dummy variable.\n",
        "\n",
        "*Note: One-hot encoding usually helps, but it can varies on a case-by-case basis. Do not hesitate to test the effect of HO encoding on your model to see if you need it.*\n",
        "\n",
        "### There is a verry simple way to do hot-one encoding in python, the pandas get_dummies function creates hot-one encoding for all categorical features of a dataset. By adding the argument drop_first=True, we drop one column for each dummy variable to dummy encode:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cd8757a652f90674dfb6972f99c3b55a225a397",
        "id": "R_YuVC-X8muN"
      },
      "cell_type": "code",
      "source": [
        "titanic_df = pd.get_dummies(data=titanic_df, drop_first=True)\n",
        "titanic_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ea021e657727b1e65c9cada661ed5ae492c62202",
        "id": "MJuUm4Jq8muN"
      },
      "cell_type": "markdown",
      "source": [
        "## **4. Feature scaling** <a id=\"scaler\"></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "ba96a06d76b3a858459486ceb0af2340a7799074",
        "id": "4b9sAO8M8muN"
      },
      "cell_type": "markdown",
      "source": [
        "### Finally, we need to perform normalization on the data. Normalizing the data is necessary because feeding a machine learning model with large or heterogeneous values can trigger large gradient updates that will prevent the gradient descent algorithm from converging. Let's look at the ranges of values for our dataframe:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hw5lbp0a8muN"
      },
      "cell_type": "code",
      "source": [
        "ranges = titanic_df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Deck', 'IsChild']].max().to_frame().T\n",
        "ranges.iplot(kind='bar', xTitle='Features', yTitle='Range', title='Range of feature before scaling')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pxWDqCRA8muN"
      },
      "cell_type": "markdown",
      "source": [
        "### Ranges are very heterogeneous. One way to change this is by using features scaling. Features scaling will set each column mean to 0 and each column variance to 1. In python, the StandarScaler class of the scikit-learn module allows us to do it vey easily:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46fdee99c06403f18ef1201671c3c8903193e8c4",
        "id": "6VPiiOOq8muN"
      },
      "cell_type": "code",
      "source": [
        "X = titanic_df.drop(['Survived'], 1)\n",
        "y = titanic_df['Survived']\n",
        "\n",
        "# Feature scaling of our data\n",
        "sc = StandardScaler()\n",
        "X = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "926c3da68404487f08462c699bb4725d819578b9",
        "id": "BSFFFNZO8muO"
      },
      "cell_type": "markdown",
      "source": [
        "### And ... we are done ! Our dataset is finally ready to go into a machine learning algorithm ! Don't forget to check my two others kernel for this dataset:\n",
        "\n",
        "- [**Complete Titanic tutorial with ML, NN & Ensembling**](https://www.kaggle.com/nhlr21/complete-titanic-tutorial-with-ml-nn-ensembling)\n",
        "- [**Titanic colorful EDA**](https://www.kaggle.com/nhlr21/titanic-colorful-eda)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}